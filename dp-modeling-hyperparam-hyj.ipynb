{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create python codes and container images for SageMaker Pipeline\n",
    "## Steps\n",
    "0. Setup Path for Data and Codes on S3\n",
    "1. Search prebuilt image list\n",
    "2. Container creation and upload to Amazon ECR\n",
    "3. local ML development and remote training job with Amazon SageMaker\n",
    "4. Generate preprocess.py for preprocessing and copy code file to S3\n",
    "5. Generate training.py for train job\n",
    "6. Generate evaluate code for evaluate processing job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Setup Path for Data and Codes on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(INPUT_DATA)\n",
    "import pandas as pd\n",
    "df = pd.read_csv(INPUT_DATA, index_col = 0)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: If you need new Docker Image, Build and upload to Amazon ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a SageMaker-compatible Catboost container\n",
    "We derive our dockerfile from the SageMaker Scikit-Learn dockerfile https://github.com/aws/sagemaker-scikit-learn-container/blob/master/docker/0.20.0/base/Dockerfile.cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending the container to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build_and_push.sh\n",
    "\n",
    "REPO_NAME=$1\n",
    "\n",
    "\n",
    "sm-docker build --repository $REPO_NAME:latest .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! bash build_and_push.sh $ecr_repository_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print('ECR container ARN: {}'.format(container_image_uri))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The docker image is now pushed to ECR and is ready for consumption! In the next section, we go in the shoes of an ML practitioner that develops a Catboost model and runs it remotely on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: local ML development and remote training job with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We install catboost locally for local development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "We use pandas to process a small local dataset into a training and testing piece.\n",
    "\n",
    "We could also design code that loads all the data and runs cross-validation within the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp s3://drivingdata/data_raw/df_dataset.csv $INPUT_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Generate preprocess.py for preprocessing and copy code file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger.debug(\"Starting preprocessing.\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-data\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    pathlib.Path(f\"{base_dir}/data\").mkdir(parents=True, exist_ok=True)\n",
    "    input_data = args.input_data\n",
    "    #input_data = \"s3://drivingdata/data_raw/df_dataset.csv\"\n",
    "    bucket = input_data.split(\"/\")[2]\n",
    "    key = \"/\".join(input_data.split(\"/\")[3:])\n",
    "\n",
    "    logger.info(\"Downloading data from bucket: %s, key: %s\", bucket, key)\n",
    "    fn = f\"{base_dir}/data/df_dataset.csv\"\n",
    "\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3.Bucket(bucket).download_file(key, fn)\n",
    "    \n",
    "    logger.debug(\"Reading downloaded data.\")\n",
    "    df_dataset = pd.read_csv(fn, index_col = 0)\n",
    "    \n",
    "    test_size=0.3\n",
    "    random_state=42\n",
    "    \n",
    "    df_dataset['Y'] = df_dataset['driving_style']\n",
    "    df_dataset= df_dataset.drop('driving_style', axis=1)\n",
    "    \n",
    "    df_train, df_test = train_test_split(df_dataset, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    \n",
    "    df_test, df_valid = train_test_split(df_test, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    if not os.path.exists(f\"{base_dir}/train/\"):\n",
    "        os.makedirs(f\"{base_dir}/train/\")\n",
    "    if not os.path.exists(f\"{base_dir}/test/\"):\n",
    "        os.makedirs(f\"{base_dir}/test/\")\n",
    "    if not os.path.exists(f\"{base_dir}/validation/\"):\n",
    "        os.makedirs(f\"{base_dir}/validation/\")\n",
    "    \n",
    "    df_train.to_csv(f\"{base_dir}/train/train.csv\", index=False)\n",
    "    df_test.to_csv(f\"{base_dir}/test/test.csv\", index=False)\n",
    "    df_valid.to_csv(f\"{base_dir}/validation/validation.csv\", index=False)\n",
    "    logger.info(\"finish data preprocessing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test preprocess Script Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $INPUT_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local test\n",
    "! python preprocess.py \\\n",
    "    --input-data $INPUT_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /opt/ml/processing/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /opt/ml/processing/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /opt/ml/processing/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /opt/ml/processing/test/test.csv /root/mmspml-spm/notebooks/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp preprocess.py $PREPROCESS_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy local test file to s3 (for the test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp /opt/ml/processing/train/train.csv $DATA_BUCKET_PREFIX/data/train/\n",
    "!aws s3 cp /opt/ml/processing/test/test.csv $DATA_BUCKET_PREFIX/data/test/\n",
    "!aws s3 cp /opt/ml/processing/validation/validation.csv $DATA_BUCKET_PREFIX/data/validation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Generate training.py for train job\n",
    "### Developing a local training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $TRAINING_PROGRAM\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    print('extracting arguments')\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
    "    parser.add_argument('--train-file', type=str, default='train.csv')\n",
    "    parser.add_argument('--validation-file', type=str, default='validation.csv')\n",
    "    parser.add_argument('--model-name', type=str, default='xgboost_classifiation_model.dump')\n",
    "#     parser.add_argument('--features', type=str)  # in this script we ask user to explicitly name features\n",
    "    parser.add_argument('--target', type=str) # in this script we ask user to explicitly name the target\n",
    "    \n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    logging.info('reading data')\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file), index_col = 0)\n",
    "    validation_df = pd.read_csv(os.path.join(args.validation, args.validation_file), index_col = 0)\n",
    "\n",
    "    print(train_df.head(5))\n",
    "    logging.info('building training and testing datasets')\n",
    "    X_train = train_df.drop(columns=[args.target])\n",
    "    X_validation = validation_df.drop(columns=[args.target])\n",
    "    y_train = train_df[args.target]\n",
    "    y_validation = validation_df[args.target]\n",
    "        \n",
    "    # define and train model\n",
    "    model = XGBClassifier(objective=\"multi:softprob\", random_state=42, eval_metric=[\"auc\"], n_estimators=500)\n",
    "    \n",
    "    model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_validation, y_validation)])\n",
    "\n",
    "    \n",
    "    # print abs error\n",
    "    logging.info('validating model')\n",
    "#     abs_err = np.abs(model.predict(X_validation) - y_validation)\n",
    "    y_pred = model.predict(X_validation)\n",
    "    accuracy_score = accuracy_score(y_validation, y_pred)\n",
    "\n",
    "    # print couple perf metrics\n",
    "    for q in [10, 50, 90]:\n",
    "        logging.info('AE-at-' + str(q) + 'th-percentile: '\n",
    "              + str(np.percentile(a=accuracy_score, q=q)))\n",
    "    \n",
    "    # persist model\n",
    "    path = os.path.join(args.model_dir, args.model_name)\n",
    "    logging.info('saving to {}'.format(path))\n",
    "    model.save_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our script locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# local test\n",
    "! python $TRAINING_PROGRAM \\\n",
    "    --train /opt/ml/processing/train/ \\\n",
    "    --validation /opt/ml/processing/validation/ \\\n",
    "    --model-dir ./ \\\n",
    "    --target Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote training in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Launch a SageMaker training job from code uploaded to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that option, we first need to send code to S3. This could also be done automatically by a build system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar zcvf source.tar.gz $TRAINING_PROGRAM\n",
    "!aws s3 cp source.tar.gz $TRAINING_PROGRAM_SUBMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then launch a training job with the `Estimator` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "output_path = DATA_BUCKET_PREFIX + '/training_jobs'\n",
    "\n",
    "\n",
    "estimator = Estimator(image_uri=TRAINING_DOCKER_IMAGE,\n",
    "                      role=role,\n",
    "                      instance_count=1,\n",
    "                      instance_type=TRAINING_INSTANCE_TYPE,\n",
    "                      output_path=output_path,\n",
    "                      hyperparameters={'sagemaker_program': TRAINING_PROGRAM,\n",
    "                                       'sagemaker_submit_directory': TRAINING_PROGRAM_SUBMIT,\n",
    "                                       'target': 'Y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location = DATA_BUCKET_PREFIX + \"/data/train/\"\n",
    "validation_location = DATA_BUCKET_PREFIX + \"/data/validation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train':train_location, 'validation': validation_location}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Generate evaluate code for evaluate processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate.py\n",
    "import argparse\n",
    "import logging\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print('extracting arguments')\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--test-file', type=str, default='test.csv')\n",
    "    parser.add_argument('--model-name', type=str, default='xgboost_classifiation_model.dump')\n",
    "    parser.add_argument('--target', type=str, default='Y') # in this script we ask user to explicitly name the target\n",
    "    \n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    logger.debug(\"Starting evaluation.\")\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    logger.debug(\"Loading xgboost model.\")\n",
    "    model = XGBClassifier(objective=\"multi:softprob\", random_state=42, eval_metric=\"auc\")\n",
    "    model.load_model(args.model_name)\n",
    "\n",
    "    logger.debug(\"Reading test data.\")\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, index_col = 0)\n",
    "\n",
    "    y_test = df[args.target]\n",
    "    X_test = df.drop(columns=args.target)\n",
    "\n",
    "    logger.info(\"Performing predictions against test data.\")\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    logger.debug(\"Calculating mean squared error.\")\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    std = np.std(y_test - predictions)\n",
    "    report_dict = {\n",
    "            \"binary_classification_metrics\": {\n",
    "                \"accuracy\": {\n",
    "                    \"value\": accuracy,\n",
    "                    \"standard_deviation\": std\n",
    "                    },\n",
    "                },\n",
    "        }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logger.info(\"Writing out evaluation report with accuracy: %f\", accuracy)\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "latest_job = client.list_training_jobs()[\"TrainingJobSummaries\"][0]\n",
    "job_desc = client.describe_training_job(TrainingJobName=latest_job[\"TrainingJobName\"])\n",
    "#print(job_desc)\n",
    "\n",
    "MODEL_PATH = job_desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "print(MODEL_PATH)\n",
    "#!aws sagemaker list-training-jobs --output text --max-items 1 --query \"TrainingJobSummaries[*].TrainingJobName\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $MODEL_PATH /opt/ml/processing/model/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /opt/ml/processing/evaluation/evaluation.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp evaluate.py $EVALUATE_CODE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $DATA_BUCKET_PREFIX/data/test/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor \n",
    "from sagemaker.processing import ProcessingInput\n",
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "\n",
    "input_data = \"/opt/ml/processing/test/test.csv\"\n",
    "output_data = \"/opt/ml/processing/output\"\n",
    "s3_model_path = MODEL_PATH\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                image_uri=EVALUATE_DOCKER_IMAGE,\n",
    "                role=ROLE_ARN,\n",
    "                instance_count=1,\n",
    "                instance_type='ml.m5.xlarge')\n",
    "\n",
    "script_processor.run(code='evaluate.py',\n",
    "                      inputs=[\n",
    "                          ProcessingInput(\n",
    "                        source= DATA_BUCKET_PREFIX + \"/data/test/test.csv\",\n",
    "                        destination='/opt/ml/processing/test',\n",
    "                        s3_data_distribution_type='ShardedByS3Key'),\n",
    "                          ProcessingInput(\n",
    "                          source=s3_model_path,\n",
    "                          destination='/opt/ml/processing/model')\n",
    "                      ],\n",
    "                      outputs=[ProcessingOutput(destination=output_data,\n",
    "                                                source='/opt/ml/processing/output_data',\n",
    "                                                s3_upload_mode = 'Continuous')],\n",
    "                         \n",
    "                      arguments=['--target', \"Y\"]\n",
    "                     )\n",
    "script_processor_job_description = script_processor.jobs[-1].describe()\n",
    "print(script_processor_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
